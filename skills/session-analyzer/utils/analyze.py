#!/usr/bin/env python3
"""
ì„¸ì…˜ ë°ì´í„° ë¶„ì„ ìœ í‹¸ë¦¬í‹°

íŒŒì‹±ëœ ì„¸ì…˜ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ê¸°ìˆ  ìŠ¤íƒ, ì‘ì—… ìœ í˜•, thinking ë¸”ë¡ì„ ë¶„ì„í•˜ê³ 
JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python parse_jsonl.py --date 2026-02-11 | python analyze.py
"""

import json
import sys
import re
from collections import Counter
from typing import List, Dict, Any


# í‚¤ì›Œë“œ ì •ì˜
LANGUAGE_KEYWORDS = [
    'python', 'javascript', 'typescript', 'java', 'go', 'rust',
    'solidity', 'haskell', 'kotlin', 'swift', 'c++', 'c#',
    'ruby', 'php', 'scala', 'elixir', 'dart', 'cpp', 'csharp',
]

FRAMEWORK_KEYWORDS = [
    'react', 'vue', 'angular', 'svelte', 'solid',
    'next.js', 'nextjs', 'nuxt', 'gatsby', 'astro', 'qwik',
    'django', 'fastapi', 'flask', 'express', 'nest.js', 'nestjs',
    'spring', 'laravel', 'rails',
]

LIBRARY_KEYWORDS = [
    'tailwind', 'shadcn', 'radix-ui', 'radix', 'mui', 'material-ui',
    'axios', 'fetch', 'graphql', 'prisma', 'drizzle',
    'jest', 'vitest', 'pytest', 'mocha', 'chai',
]

TASK_TYPE_KEYWORDS = {
    'ğŸ’» Coding': [
        'êµ¬í˜„', 'ì‘ì„±', 'ê°œë°œ', 'ë§Œë“¤', 'ìƒì„±', 'implement', 'create', 'build', 'develop', 'add'
    ],
    'ğŸ› Debugging': [
        'ì—ëŸ¬', 'ì˜¤ë¥˜', 'ë²„ê·¸', 'ê³ ì¹˜', 'ìˆ˜ì •', 'error', 'bug', 'fix', 'debug', 'issue'
    ],
    'â™»ï¸ Refactoring': [
        'ë¦¬íŒ©í† ë§', 'ê°œì„ ', 'ìµœì í™”', 'refactor', 'optimize', 'improve', 'cleanup'
    ],
    'âœ… Testing': [
        'í…ŒìŠ¤íŠ¸', 'ê²€ì¦', 'í™•ì¸', 'test', 'verify', 'check', 'validate'
    ],
    'ğŸ“š Learning': [
        'ê³µë¶€', 'í•™ìŠµ', 'ì´í•´', 'ì•Œì•„ë³´', 'learn', 'study', 'explore', 'understand'
    ],
    'ğŸ“‹ Planning': [
        'ê³„íš', 'ì„¤ê³„', 'ëª…ì„¸', 'plan', 'design', 'spec', 'documentation', 'ì•„í‚¤í…ì²˜'
    ],
    'âš™ï¸ Configuration': [
        'ì„¤ì •', 'í™˜ê²½', 'ì„¤ì¹˜', 'config', 'setup', 'install', 'configure'
    ],
    'ğŸ” Research': [
        'ì¡°ì‚¬', 'ë¶„ì„', 'ì°¾ì•„', 'search', 'find', 'investigate', 'analyze'
    ],
}


def extract_keywords(text: str, keywords: List[str]) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)"""
    if not text:
        return []

    text_lower = text.lower()
    found = []
    for keyword in keywords:
        if keyword.lower() in text_lower:
            found.append(keyword)
    return found


def analyze_tech_stack(sessions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„"""
    languages = Counter()
    frameworks = Counter()
    libraries = Counter()
    file_extensions = Counter()

    for session in sessions:
        # ë©”ì‹œì§€ì™€ thinking ë¸”ë¡ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        all_text = []

        for msg in session.get('messages', []):
            content = msg.get('content', '')
            if isinstance(content, str):
                all_text.append(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and 'text' in item:
                        all_text.append(item['text'])

        for thinking in session.get('thinking_blocks', []):
            all_text.append(thinking)

        combined_text = ' '.join(all_text)

        # ì–¸ì–´ ì¶”ì¶œ
        for lang in extract_keywords(combined_text, LANGUAGE_KEYWORDS):
            languages[lang] += 1

        # í”„ë ˆì„ì›Œí¬ ì¶”ì¶œ
        for fw in extract_keywords(combined_text, FRAMEWORK_KEYWORDS):
            frameworks[fw] += 1

        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ì¶œ
        for lib in extract_keywords(combined_text, LIBRARY_KEYWORDS):
            libraries[lib] += 1

        # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
        for tool_call in session.get('tool_calls', []):
            if tool_call.get('name') in ['Read', 'Write', 'Edit']:
                file_path = tool_call.get('input', {}).get('file_path', '')
                if file_path and '.' in file_path:
                    ext = file_path.rsplit('.', 1)[-1]
                    if len(ext) <= 10:  # í™•ì¥ìê°€ ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ê²½ìš°ë§Œ
                        file_extensions[ext] += 1

    return {
        'languages': dict(languages.most_common(10)),
        'frameworks': dict(frameworks.most_common(10)),
        'libraries': dict(libraries.most_common(10)),
        'file_extensions': dict(file_extensions.most_common(10)),
    }


def classify_task_type(session: Dict[str, Any]) -> str:
    """ì„¸ì…˜ì˜ ì‘ì—… ìœ í˜• ë¶„ë¥˜"""
    messages = session.get('messages', [])

    # ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
    first_user_message = None
    for msg in messages:
        if msg.get('type') == 'user':
            first_user_message = msg.get('content', '')
            break

    if not first_user_message:
        return 'General'

    # ë¬¸ìì—´ë¡œ ë³€í™˜
    if isinstance(first_user_message, list):
        text_parts = []
        for item in first_user_message:
            if isinstance(item, dict) and 'text' in item:
                text_parts.append(item['text'])
        first_user_message = ' '.join(text_parts)

    message_lower = first_user_message.lower()

    # í‚¤ì›Œë“œ ë§¤ì¹­
    for task_type, keywords in TASK_TYPE_KEYWORDS.items():
        if any(keyword.lower() in message_lower for keyword in keywords):
            return task_type

    return 'General'


def analyze_tool_usage(sessions: List[Dict[str, Any]]) -> Dict[str, int]:
    """ë„êµ¬ ì‚¬ìš© ë¹ˆë„ ë¶„ì„"""
    tool_counter = Counter()

    for session in sessions:
        for tool_call in session.get('tool_calls', []):
            tool_name = tool_call.get('name')
            if tool_name:
                tool_counter[tool_name] += 1

    return dict(tool_counter.most_common(10))


def extract_thinking_insights(sessions: List[Dict[str, Any]], max_per_session: int = 5) -> List[str]:
    """Thinking ë¸”ë¡ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
    insights = []

    decision_keywords = ['ê²°ì •', 'ì„ íƒ', 'íŒë‹¨', 'decide', 'choose', 'select', 'option']
    problem_keywords = ['ë¬¸ì œ', 'í•´ê²°', 'ë°©ë²•', 'ì ‘ê·¼', 'problem', 'solve', 'approach', 'solution']

    for session in sessions:
        session_insights = []

        for thinking in session.get('thinking_blocks', [])[:max_per_session]:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            sentences = re.split(r'[.!?]\s+', thinking)

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # ì˜ì‚¬ê²°ì • ê´€ë ¨ ë¬¸ì¥
                if any(kw in sentence.lower() for kw in decision_keywords):
                    session_insights.append(f"[ê²°ì •] {sentence}")

                # ë¬¸ì œ í•´ê²° ê´€ë ¨ ë¬¸ì¥
                elif any(kw in sentence.lower() for kw in problem_keywords):
                    session_insights.append(f"[í•´ê²°] {sentence}")

        insights.extend(session_insights[:max_per_session])

    return insights[:20]  # ìµœëŒ€ 20ê°œ


def analyze_workflow_patterns(sessions: List[Dict[str, Any]]) -> List[str]:
    """ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ë¶„ì„"""
    patterns = Counter()

    for session in sessions:
        tool_sequence = [tc.get('name') for tc in session.get('tool_calls', []) if tc.get('name')]

        # 3ê°œì”© ë¬¶ì–´ì„œ íŒ¨í„´ ì¶”ì¶œ
        for i in range(len(tool_sequence) - 2):
            pattern = ' â†’ '.join(tool_sequence[i:i+3])
            patterns[pattern] += 1

    # ìƒìœ„ 5ê°œ íŒ¨í„´
    return [f"{pattern} ({count}íšŒ)" for pattern, count in patterns.most_common(5)]


def analyze_sessions(data: Dict[str, Any]) -> Dict[str, Any]:
    """ì „ì²´ ì„¸ì…˜ ë°ì´í„° ë¶„ì„"""
    sessions = data.get('sessions', [])

    if not sessions:
        return {'error': 'ë¶„ì„í•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.'}

    # ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„
    tech_stack = analyze_tech_stack(sessions)

    # ì‘ì—… ìœ í˜• ë¶„ë¥˜
    task_types = Counter()
    for session in sessions:
        task_type = classify_task_type(session)
        task_types[task_type] += 1

    # ë„êµ¬ ì‚¬ìš© ë¶„ì„
    tool_usage = analyze_tool_usage(sessions)

    # Thinking ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
    thinking_insights = extract_thinking_insights(sessions)

    # ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ë¶„ì„
    workflow_patterns = analyze_workflow_patterns(sessions)

    # í†µê³„ ê³„ì‚°
    total_messages = sum(len(s.get('messages', [])) for s in sessions)
    total_tool_calls = sum(len(s.get('tool_calls', [])) for s in sessions)
    avg_messages = total_messages / len(sessions) if sessions else 0
    avg_tool_calls = total_tool_calls / len(sessions) if sessions else 0

    # ì„¸ì…˜ ìƒì„¸ ì •ë³´
    session_details = []
    for session in sessions:
        task_type = classify_task_type(session)
        messages = session.get('messages', [])
        first_user_msg = next((m.get('content', '') for m in messages if m.get('type') == 'user'), '')

        # ì²« ë©”ì‹œì§€ ìš”ì•½ (ìµœëŒ€ 100ì)
        if isinstance(first_user_msg, list):
            text_parts = []
            for item in first_user_msg:
                if isinstance(item, dict) and 'text' in item:
                    text_parts.append(item['text'])
            first_user_msg = ' '.join(text_parts)

        summary = first_user_msg[:100] + '...' if len(first_user_msg) > 100 else first_user_msg

        session_details.append({
            'file_path': session.get('file_path'),
            'task_type': task_type,
            'summary': summary,
            'metadata': session.get('metadata', {}),
            'message_count': len(messages),
            'tool_call_count': len(session.get('tool_calls', [])),
        })

    return {
        'date_range': data.get('date_range', {}),
        'statistics': {
            'total_sessions': len(sessions),
            'total_messages': total_messages,
            'total_tool_calls': total_tool_calls,
            'avg_messages_per_session': round(avg_messages, 1),
            'avg_tool_calls_per_session': round(avg_tool_calls, 1),
        },
        'tech_stack': tech_stack,
        'task_types': dict(task_types.most_common()),
        'tool_usage': tool_usage,
        'thinking_insights': thinking_insights,
        'workflow_patterns': workflow_patterns,
        'session_details': session_details,
    }


def main():
    # stdinìœ¼ë¡œë¶€í„° JSON ë°ì´í„° ì½ê¸°
    try:
        input_data = json.load(sys.stdin)
    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}", file=sys.stderr)
        sys.exit(1)

    # ë¶„ì„ ìˆ˜í–‰
    result = analyze_sessions(input_data)

    # ê²°ê³¼ ì¶œë ¥
    print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == '__main__':
    main()
